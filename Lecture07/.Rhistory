#2.1
library(shape)
set.seed(0.123456)
xcoord <- runif(10000)
ycoord <- runif(10000)
z=sqrt(xcoord^2+ycoord^2)
plot(xcoord,ycoord,col='dark blue',pch=4,cex=.5)
pi.upskattning10 <- pi-4*(sum(z[1:10]<=1)/10)
pi.upskattning100 <- pi-4*(sum(z[1:100]<=1)/100)
pi.upskattning1000 <- pi-4*(sum(z[1:1000]<=1)/1000)
pi.upskattning10000 <- pi-4*(sum(z[1:10000]<=1)/10000)
print(c("pi minus uppskattat vÃ¤rde pÃ¥ fÃ¶r 10000 iterationer pi:",pi.upskattning10))
print(c("pi minus uppskattat vÃ¤rde pÃ¥ fÃ¶r 10000 iterationer pi:",pi.upskattning100))
print(c("pi minus uppskattat vÃ¤rde pÃ¥ fÃ¶r 10000 iterationer pi:",pi.upskattning1000))
print(c("pi minus uppskattat vÃ¤rde pÃ¥ fÃ¶r 10000 iterationer pi:",pi.upskattning10000))
#2.1###SVAR###
print("Uppskattningen av pi blir bÃ¤ttre ju fler iterationer man gÃ¶r.")
#2.2
#radien fÃ¶r cirkelbÃ¥garna:
r <- c(.4,.8,1)
plotcircle(r=r[1], mid=c(0,0), lwd=3,lcol="yellow")
plotcircle(r=r[2], mid=c(0,0), lwd=3,lcol="red")
plotcircle(r=r[3], mid=c(0,0), lwd=3,lcol="blue")
print("End")
#del 3 Hitta mÃ¶rdaren
library(tidyverse)
df <- read.csv('telemastdata.csv')
#Telefon typ=iPhone, time0=416+-9 dvs. 407-427
Misstankta_df<- filter(df, phone_type =='iPhone' & time0 >= 407 & time0<= 425)
View(Misstankta_df)
library(tidyverse)
df_lungcap <- read.csv("lungcap.csv")
#tib_lcap <- as_tibble(df_lungcap) <-FÃ¶r snabb koll
#tib_lcap
par(mfrow=c(7,7))
plot(df_lungcap)#Age & Height har en eller flera uppenbara outliers
View(df_lungcap)#Kikar pÃ¥ Dataframe direkt fÃ¶r att se om outliers Ã¤r helt uppenbara genom att sortera efter storlek
#Rad 89 tas bort, motivering: troligen felaktigt vÃ¤rde, Age/Height=10000,Ãvriga vÃ¤rden faller mellan 3-19 fÃ¶r Age och mellan 45,3-81,8 fÃ¶r Height
paste("Standardavvikelsen fÃ¶r Age:",Age_sd_before <- sd(df_lungcap$Age))#Standard avvikelse innan borttagning
paste("Standardavvikelsen fÃ¶r Height:",Height_sd_before <- sd(df_lungcap$Height))#Standard avvikelse innan borttagning
df_LungCap.bak <- df_lungcap#sparar Dataframe innan vÃ¤rdet tas bort dÃ¥ det Ã¤r en liten dataframe
df_lungcap <- df_lungcap[-89,]#tar bort rad 89 ur dataframen
#Matar ut den nya standardavvikelsen i procent jÃ¤mfÃ¶rt med vad den var innan borttagningen av rad 89 med tvÃ¥ decimaler
paste("Standardavvikelsen fÃ¶r Age Ã¤r nu",round((sd(df_lungcap$Age)/Age_sd_before)*100,2),"procent av vad den var innan borttagningen av raden/raderna")
paste("Standardavvikelsen fÃ¶r Height Ã¤r nu",round((sd(df_lungcap$Height)/Height_sd_before)*100,2),"procent av vad den var innan borttagningen av raden/raderna")
round((sd(df_lungcap$Height)/Height_sd_before)*100,2)
#standardavvikelsen har minskat till ca: 1,08 respektive 1,95 PROCENT av ursprungs standardavvikelsen
par(mfrow=c(7,7))#Kikar pÃ¥ hela dataframen igen
plot(df_lungcap)
###SVAR###
paste0("new_var",
which.max(
c(summary(lm(LungCap ~ Age+Height+new_var1, data = df_lungcap))$r.squared,
summary(lm(LungCap ~ Age+Height+new_var2, data = df_lungcap))$r.squared,
summary(lm(LungCap ~ Age+Height+new_var3, data = df_lungcap))$r.squared,
summary(lm(LungCap ~ Age+Height+new_var4, data = df_lungcap))$r.squared,
summary(lm(LungCap ~ Age+Height+new_var5, data = df_lungcap))$r.squared,
summary(lm(LungCap ~ Age+Height+new_var6, data = df_lungcap))$r.squared)),
" ger hÃ¶gst R-squared i kombination med Age och Height.")
pnorm(-1.625)
pnorm(1.875)
pnorm(0)
1-pnorm(7.79)
#CTG Random Forest
library(readxl)
library(dplyr)
library(tidyverse)
library(randomForest)
library(tree)
library(caret)
#data <- as_tibble(read.csv("CTG.csv"))
data <- as_tibble(read_excel("CTG.xls", sheet = "Raw Data"))
#Standardinspektion
print(data)
#Standardinspektion
print(data)
setwd("~/GitHub/visuell_data_analys/Lecture07")
#CTG Random Forest
library(readxl)
library(dplyr)
library(tidyverse)
library(randomForest)
library(tree)
library(caret)
#data <- as_tibble(read.csv("CTG.csv"))
data <- as_tibble(read_excel("CTG.xls", sheet = "Raw Data"))
#Standardinspektion
print(data)
print(colnames(data))
print(str(data))
#Vi har en del NA-värden. För mycket data att traggla sig igenom.
print(sum(is.na(data)))
print(dim(data)[1]*dim(data)[2])
boolean_df <- as.data.frame(rowSums(is.na(data)))
colnames(boolean_df) <- "na_rows"
print(filter(boolean_df,na_rows > 0))
#Verkar bara ha att göra med 4 observationer med MÅNGA NA-värden. Vi omittar.
data <- na.omit(data)
print(str(data))
#NSP är det vi söker och vill göra predictions på
data$NSP <- factor(data$NSP)
#Detta innebär att vi har 1655 är friska, 295 är misstänkta, 176 sjuka
table(data$NSP)
#Genom att sätta seed kan vi garantera att ha samma resultat varje gång,
#även vid slumpade försök
set.seed(2)
#Vi delar upp i 70% train och 30% test set med bootstrapping
indices <- sample(2, nrow(data), replace = TRUE, prob = c(0.7, 0.3))
rows <- sample(nrow(data))
train <- data[indices == 1, ]
test <- data[indices == 2, ]
#Här säger vi att vi vill bestämma NSP med ALLA våra variabler
forest_classifier <- randomForest(NSP ~., data = train)
#Vi kan kolla lite matnyttig information här
print(forest_classifier)
predictions <- predict(forest_classifier,train)
#Vi kan jämföra head av train med predictions
head(predictions)
head(train$NSP)
#Vi kan även kalla på confusionmatrix såhär
confusionMatrix(predictions, train$NSP)
#Ser snabbt att det minsann går att få lite fel!
real_predictions <- predict(forest_classifier, test)
head(real_predictions)
head(test$NSP)
confusionMatrix(real_predictions, test$NSP)
#Error rates av random forest:
#Vi får summering av vår classifier där vi kan se att vi egentligen inte hade behövt
#fler än cirka 150 trees.
plot(forest_classifier)
#Ger oss importance för varje feature
varImpPlot(forest_classifier)
#Vi kan också intressera oss för de 10 viktigaste
varImpPlot(forest_classifier, sort = TRUE, n.var = 10)
library(randomForest)
library(tree)
library(ggplot2)
library(GGally)
library(dplyr)
iris %>% head()
iris %>% head()
head(iris)
decision_tree <- tree(Species ~ ., data = iris) # Interpretation
# 1. use tree function
# 2. sort species
# 3. based on all(.) variables
# 4. data is iris dataset
decision_tree
plot(decision_tree)
text(decision_tree)
ggpairs(iris[,1:5])
index_row <- sample(2,
nrow(iris),
replace = T,
prob = c(0.7, 0.3)
)                 #assign values to the rows (1: Training, 2: Test)
train_data <- iris[index_row == 1,]
test_data <- iris[index_row == 2,]
iris_classifier <- randomForest(Species ~.,
data = train_data, #train data set
importance = T)
iris_classifier
plot(iris_classifier)
importance(iris_classifier)
varImpPlot(iris_classifier)
varImpPlot(iris_classifier)
qplot(Petal.Width, Petal.Length, data=iris, color = Species)
qplot(Sepal.Width, Sepal.Length, data=iris, color = Species)
predicted_table <- predict(iris_classifier, test_data[,-5])
table(observed = test_data[,5], predicted = predicted_table)
